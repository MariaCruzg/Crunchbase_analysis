# -*- coding: utf-8 -*-
"""cleaning_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14teumltmzmo5crK34absfuXNZJOM-S4G
"""

# -----------------------------------------------------------
# Create an  algorithm to  presented classify the status of the companies (open, acquired or closed )
# based on the market
# (C) 2020 Maria Cruz,  Mexico
# Released under GNU Public License (GPL)
# email mcruzg.cg@gmail.com 
# -----------------------------------------------------------

import numpy as np   #library to high-performance multidimensional array object 
import pandas as pd. # ibrary to analysis 
from google.colab import drive #library to conect to to google driv
 from wordcloud import WordCloud #library to visualization 
import matplotlib.pyplot as plt  #library to visualization 
import seaborn as sns #library to visualization  
from sklearn.ensemble import RandomForestClassifier # MLA Library 
from sklearn.model_selection import train_test_split# MLA Library 
from sklearn.model_selection import cross_val_score# MLA Library 
from sklearn.metrics import classification_report, confusion_matrix # MLA Library 
from sklearn.metrics import roc_curve, auc # MLA Library 
from sklearn.preprocessing import LabelEncoder #MLA Library 
from keras.models import Sequential #Deep Learning Library 
from keras.layers import Dense #Deep Learning Library 
from keras.optimizers import SGD,Adam #Deep Learning Library

#------------------------- READ FILES --------------------------
drive.mount('/content/drive') #mount data from drive 
!ls "/content/drive/My Drive/umba" #reading the file xls
data = pd.read_excel('/content/drive/My Drive/umba/crunchbase_monthly_export_d43b4klo2ade53.xlsx',sheet_name=None) #xls to data frame 
odict_keys=['Companies', 'Rounds', 'Investments', 'Acquisitions'] #names of the sheets
data1=data[odict_keys[0]]

#------------------------- data cleaning ------------------------
category=data1['status'].astype('category') # y variable to describe
data1['founded_at'] = pd.to_datetime(data1['founded_at'], errors = 'coerce' ) #from object to datetime

data_test_model= data1[['market','funding_total_usd','funding_rounds','status']] #select the variables to the model

features=data_test_model.replace(np.nan,-9, regex=True) #avoid nan
features['market'] = features['market'].str.replace('|', ' ') #cleaning data 
features['market'] = features['market'].str.replace('+', ' ') #cleaning data 
features['market'] = features['market'].str.replace('&', ' ') #cleaning data 
features['market'] = features['market'].str.replace('>', ' ') #cleaning data 
features['market'] = features['market'].str.replace('<', ' ') #cleaning data  
eatures['market']=features['market'].astype("string") # from object to string

features= features.drop(['status'], axis = 1) #delete the y variable from the dataset

#---------------------- Feature engineering -----------------------------
market=pd.get_dummies(features['market'],dummy_na=True)

result = pd.concat([features, market], axis=1, sort=False)

features=features.fillna(0) #fill NaN  
category= (category.cat.codes.replace(-1, np.nan)  #fill NaN on text 
               .interpolate().astype(int).astype('category')
               .cat.rename_categories(category.cat.categories))

#------------------ Model ----------------------------------------------
y = pd.factorize(category)[0] #from string to numeric 
encoder =  LabelEncoder() #encoder data 
y1 = encoder.fit_transform(y)
Y = pd.get_dummies(y1).values  #restructure  y data 
Xtrain,Xtest, ytrain,ytest = train_test_split(features,Y,test_size=0.2,random_state=0)  #80% training 20% test

#-------------------- Model Parameters---------------------------------

model = Sequential() #call sequential model
model.add(Dense(10,input_shape=(756,),activation='tanh')) #number of the parameters from X
model.add(Dense(8,activation='tanh'))
model.add(Dense(6,activation='tanh'))
model.add(Dense(3,activation='softmax')) 
model.compile(Adam(lr=0.04),'categorical_crossentropy',metrics=['accuracy'])
model.summary()

#----------------Train --------------------------------
model.fit(Xtrain,ytrain,epochs=100)
# ----------- Predict on test   --------------------------------
y_pred = model.predict(Xtest)
y_test_class = np.argmax(ytest,axis=1)
y_pred_class = np.argmax(y_pred,axis=1)

#--------------  save model to production -------------------
# save the model to disk
filename = 'finalized_model.sav'
pickle.dump(model, open(filename, 'wb'))
 
# some time later...

# load the model from disk
loaded_model = pickle.load(open(filename, 'rb'))
result = loaded_model.score(X_test, Y_test)